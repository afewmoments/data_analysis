---
title: "Hotel Bookings"
author: "Stas Masiuta"
date: '`r Sys.Date()`'
output: github_document
---

# Libraries

```{r}
library(tidyverse)
library(tidymodels)
library(skimr)      # вместо glimpse()
library(GGally)     # парные графики
```

# Import data

```{r}
hotels <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv")
```

[more info about data](https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-02-11)

# Explore data


## Clean data

```{r}
hotels %>% summary()
```

Объединим группу детей (children and babies) --- в бинарный классификатор, есть дети нет детей.
Тоже самое сделаем с парковочными местами.
Оставим только те записи, в которых не было отмены брони.


```{r}
hotel_stays <- hotels %>%
  filter(is_canceled == 0) %>%
  mutate(
    children = case_when(
      children + babies > 0 ~ "children",
      TRUE ~ "none"
    ),
    required_car_parking_spaces = case_when(
      required_car_parking_spaces > 0 ~ "parking",
      TRUE ~ "none"
    )
  ) %>%
  select(-is_canceled, -reservation_status, -babies)
```

## Analyze data

Без детей останавливались почти в 10 раз больше

```{r}
hotel_stays %>%
  count(children)
```

Выведем обзорную информацию о данных

```{r}
hotel_stays %>% 
  skim()
```

Здесь большой разброс численных характеристик, их нужно будет нормировать.
Все текстовые параметры нужно будет перевести в фактор.

### В какие месяцы останавливаются чаще

```{r}
hotel_stays %>%
  mutate(arrival_date_month = factor(arrival_date_month,
    levels = month.name
  )) %>%
  count(hotel, arrival_date_month, children) %>%
  group_by(hotel, children) %>%
  mutate(proportion = n / sum(n)) %>%
  ggplot(aes(arrival_date_month, proportion, fill = children)) +
  geom_col(position = "dodge") +
  scale_y_continuous(labels = scales::percent_format()) +
  facet_wrap(~hotel, nrow = 2) +
  labs(
    x = NULL,
    y = "Proportion of hotel stays",
    fill = NULL
  )
```

### В какие месяцы останавливается больше гостей

```{r}
hotel_stays %>% count(deposit_type)
```

```{r}
hotel_stays %>% count(adults)
```

### Гости с детьми предпочитают бронировать парковку чаще чем бездетные?

```{r}
hotel_stays %>%
  count(hotel, required_car_parking_spaces, children) %>%
  group_by(hotel, children) %>%
  mutate(proportion = n / sum(n)) %>%
  ggplot(aes(required_car_parking_spaces, proportion, fill = children)) +
  geom_col(position = "dodge") +
  scale_y_continuous(labels = scales::percent_format()) +
  facet_wrap(~hotel, nrow = 2) +
  labs(
    x = NULL,
    y = "Proportion of hotel stays",
    fill = NULL
  )
```

Ну немного чаще да.

Теперь построим сразу много разных парных графиков

```{r}
hotel_stays %>%
  select(
    children, adr,
    required_car_parking_spaces,
    total_of_special_requests
  ) %>%
  GGally::ggpairs(aes(color = children))
```

Как распределена цена за сутки для брони с детьми и без:

```{r}
hotel_stays %>% 
  ggplot(aes(adr, fill = children)) +
  geom_histogram() +
  facet_wrap(~children)
```


# Build model

## Make data for model

Отберём интересные переменные, а так же переведём все номинативные переменные в фактор.

```{r}
hotels_df <- hotel_stays %>%
  select(
    children, hotel, arrival_date_month, meal, adr, adults,
    required_car_parking_spaces, total_of_special_requests,
    stays_in_week_nights, stays_in_weekend_nights
  ) %>%
  mutate_if(is.character, factor)
```


## Разобъём датасет

```{r}
set.seed(1234)
hotel_split <- initial_split(hotels_df)

hotel_train <- training(hotel_split)
hotel_test <- testing(hotel_split)
```


## Создадим рецепт

В нём мы сделали следующее
  * step_downsample() --- приведём количество классов к балансному значению. Уменьшаем выборку данных, поскольку без детей остается примерно в 10 раз больше, чем с проживанием в отеле. Если мы этого не сделаем, наша модель очень эффективно научится предсказывать отрицательный случай
  * step_dummy() --- дамми переменные для всех номинативных переменных, за исключением предиката.
  * step_zv() --- удаляем все числовые переменные с нулевой дисперсией.
  * step_normalize() --- В качестве последнего шага мы нормализуем (центрируем и масштабируем) числовые переменные.

```{r}
hotel_rec <- recipe(children ~ ., data = hotel_train) %>%
  step_downsample(children) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_numeric()) %>%
  step_normalize(all_numeric()) %>%
  prep()

hotel_rec
```


## Применим рецепт к тестовым данным

Это делается для того, чтобы тестовые данные были подготовлены так же как и тренировочные. На них проделаны все теже самые действия.

```{r}
test_proc <- bake(hotel_rec, new_data = hotel_test)
```

## Тренируем модель

Будем обучать несколько моделей.
Алгоритм ближайших соседей и дерево решений.

### knn model

```{r}

# задаём движок
knn_spec <- nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_fit <- knn_spec %>%
  fit(children ~ ., data = juice(hotel_rec))

knn_fit
```

### tree model

```{r}
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_fit <- tree_spec %>%
  fit(children ~ ., data = juice(hotel_rec))

tree_fit
```

# Evaluate model

Для того чтобы оценить выборку создадим кросс-валидационный набор.

```{r}
set.seed(1234)
validation_splits <- mc_cv(juice(hotel_rec), prop = 0.9, strata = children)

```

Теперь каждый из сэмплов мы нафитим нашей моделью

```{r}
knn_res <- fit_resamples(
  knn_spec,
  children ~ .,
  validation_splits,
  control = control_resamples(save_pred = TRUE)
)

knn_res %>%
  collect_metrics()
```


```{r}
tree_res <- fit_resamples(
  tree_spec,
  children ~ .,
  validation_splits,
  control = control_resamples(save_pred = TRUE)
)

tree_res %>% 
  recall(splits, .predictions)
  collect_metrics()
```


```{r}
knn_res %>%
  unnest(.predictions) %>%
  mutate(model = "kknn") %>%
  bind_rows(tree_res %>%
    unnest(.predictions) %>%
    mutate(model = "rpart")) %>%
  group_by(model) %>%
  roc_curve(children, .pred_children) %>% View
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_line(size = 1.5) +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  )
```


Судя по графикам, алгоритм kknn справился с задачей лучше чем деревью решений

Ну и в качестве финальных метрик можно посчитать остальные статистики:

```{r}
knn_conf <- knn_res %>%
  unnest(.predictions) %>%
  conf_mat(children, .pred_class)
```
