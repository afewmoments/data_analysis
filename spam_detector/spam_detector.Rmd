---
title: "spam_detector"
output: html_document
---

# Lybraries

```{r}
library(tidyverse)
library(tm)
```


# Основной подход

Используем наивный байесовский классификатор.
Логика его работы такая: если видим слово, которое в спаме встречается чаще, чем в не‐спаме, то кладем его в копилку спам‐признаков. По такомуже принципу формируем копилку признаков для не‐спама.


Как эти признаки помогут нам отделять спам от не‐спама? Мы ищем в анализируемом письме оба вида признаков. Если в итоге получается, что признаков спама больше, чем признаков не‐спама, значит письмо спамное, иначе — правомерное.

Вычисляя веро ятности того, спам ли наше письмо, мы не учитываем, что какие‐то сло ва могут быть взаимозависимыми. Мы оцениваем каждое слово в отрыве от  всех остальных слов. На статистическом сленге такой подход
называется «статистической независимостью». Когда математики‐статистики
исходят из такого предположения, не будучи до конца уверенными в том, что
оно здесь правомерно, они говорят: «Наша модель наивная». Отсюда и название: наивный байесовский классификатор, а не просто байесовский классификатор.


# Функция чтения писем из файлов

```{r}
spam_learn.path <- file.path("data", "spam_learn")
spam_verify.path <- file.path("data", "spam_verify")
easy_nonspam_learn.path <- file.path("data", "easy_nonspam_learn")
easy_nonspam_verify.path <- file.path("data", "easy_nonspam_verify")
hard_nonspam_verify.path <- file.path("data", "hard_nonspam_verify")
```


Каждый отдельно взятый файл с письмом состоит из двух блоков: заголовок
с метаданными и содержание письма. Первый блок отделен от второго пустой стро кой (это  особенность протокола электронной почты описана в RFC822). Метаданные нам не нужны. Нас интересует  только содержимое письма. Поэтому напишем функцию, которая считывает его из файла с письмом.

```{r}
getMessage <- function(path) {
  
  con <- file(path, open = "rt", encoding = "latin1")
  text <- readLines(con)
  
  # Текстовое послание всегда начинается с пустой строки
  #msg — это и есть содержимое письма, без заголовочных метаданных.
  msg <- text[seq(ifelse(is.na((which(text == "")[1])), 1, which(text == "")[1] + 1), length(text), 1)]
  close(con)
  
  return(paste(msg, collapse = "\n"))
}

test_path <- "data/spam_learn/0000.7b1b73cf36cf9dbc3d64e3f2ee2b91f1"
con <- file(test_path, open = "rt", encoding = "latin1")
text <- readLines(con)
text[seq(ifelse(is.na((which(text == "")[1])), 1, which(text == "")[1] + 1), length(text), 1)]
seq(as.numeric(which(text == "")[1] + 1), length(text), 1)
which(text[1] == "")



```

Каждый отдельно взятый элемент  вектора — это  отдельное письмо


# Готовим тренировочные данные из спамного корпуса текстов

```{r}
# загружаем все спамные письма в единый вектор
spam_learn.docs <- dir(spam_learn.path)
spam_learn.docs <- spam_learn.docs[which(spam_learn.docs != "cmds")]

all_spam.learn <- sapply(head(spam_learn.docs,1), 
                         function(p) getMessage(file.path(head(spam_learn.path,1), p)))
```



# Готовим корпус текстов для спамных писем
В корпусной лингвистике составные части текста, в том числе слова, называют термами

Нам надо создать терм-документную матрицу(TDM), у которой N строк и M столбцов:
N – количество  уникальных термов, найденных во  всех документах; 
M — количество  документов в корпусе текстов). 
Ячейка [iTerm, jDoc] указывает, сколько раз терм с номером iTerm встречается в письме с номером jDoc.

```{r}
getDTM <- function(doc.vec) {
  control <- list(stopwords = TRUE,
                  removePunctuation = TRUE,
                  removeNumbers = TRUE,
                  minDocFreq = 2)
  doc.corpus <- Corpus(VectorSource(doc.vec))
  doc.dtm <- TermDocumentMatrix(doc.corpus, control)
  return(doc.dtm)
}
```










