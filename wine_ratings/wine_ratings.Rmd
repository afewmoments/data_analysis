---
title: "Wine ratings"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Подготовка данных

```{r}
library(tidyverse)
library(broom)
library(tidytext)
library(widyr)
theme_set(theme_light())
```


```{r}
wine_ratings <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-28/winemag-data-130k-v2.csv") %>% 
  select(-X1) %>% 
  #  добавляем новую переменную год производства, её вытаскиваем из названия вина
  extract(title, "year", "(20\\d\\d)", convert = T, remove = FALSE) %>% 
  mutate(year = ifelse(year < 1900, NA, year)) %>% 
  filter(!is.na(price))
```


```{r}
wine_ratings %>% View()
  count(country, sort = TRUE)

# посмотрим разные разрезы данных
wine_ratings %>% 
  count(designation, sort = TRUE)

wine_ratings %>% 
  count(taster_name, sort = TRUE)

wine_ratings %>% 
  filter(!is.na(designation)) %>% 
  count(variety, designation, sort = TRUE)

wine_ratings %>% 
  ggplot(aes(points)) +
  geom_histogram(binwidth = 1)

wine_ratings %>% 
  ggplot(aes(year)) +
  geom_histogram()

wine_ratings %>% 
  ggplot(aes(price)) +
  geom_histogram() +
  scale_x_log10()
```

Теперь посмотрим распредение вина в зависимости от цены и оценки 

```{r}
ggplot(wine_ratings, aes(price, points)) + 
  geom_point(alpha = .1) +
  geom_smooth(method = "lm") + 
  scale_x_log10()
```

На этом же графике построена линейный тренд.

```{r}
summary(lm(points ~ log2(price), wine_ratings))
```

Что нам говорят результаты предсказания? Фактически, что каждый раз, когда цена `price` удваивается, ожидаемое количество очков `points` увеличивается на два.

Теперь постараемся улучшить нашу предсказательную модель. Добавим в неё параметр страны производителя.


```{r}
wine_ratings %>% 
  mutate(country = fct_relevel(fct_lump(country, 7), "US")) %>%
  mutate(country = fct_reorder(country, points)) %>% 
  ggplot(aes(country, points)) +
  geom_boxplot() +
  coord_flip()

wine_ratings %>% 
  mutate(country = fct_relevel(fct_lump(country, 7), "US")) %>% 
  lm(points ~ log2(price) + country, data = .) %>% 
  summary()
```

Посмотрим теперь, будет ли значимым добавление параметра года производства:

```{r}
wine_ratings %>%
  group_by(year) %>% 
  summarize(average_point = mean(points), n())
```

В принципе год встречается довольно много раз, и от него может что-то зависеть, посмотрим как изменится предсказательная модель

```{r}
wine_ratings %>% 
  mutate(country = fct_relevel(fct_lump(country, 7), "US")) %>% 
  lm(points ~ log2(price) + country + year, data = .) %>% 
  summary()
```


Да, действительно --- параметр год производства, статистически значим p-value < 0.05.

Попробуем теперь добавить в модель параметр "кто обозревал", т.е. `taster_name`

```{r}
wine_ratings %>%
  mutate(taster_name = fct_reorder(fct_lump(taster_name, 10), points)) %>% 
  ggplot(aes(taster_name, points)) +
  geom_boxplot() +
  coord_flip()
```

```{r}
wine_ratings %>% 
  replace_na(list(taster_name = "Missing")) %>% 
  mutate(country = fct_relevel(fct_lump(country, 7), "US")) %>% 
  mutate(taster_name = fct_relevel(fct_lump(taster_name, 6), "Missing")) %>% 
  lm(points ~ log2(price) + country + year + taster_name, data = .) %>% 
  summary()
```

Теперь построим линейную модель

```{r}
model <- wine_ratings %>% 
  replace_na(list(taster_name = "Missing", country = "Missing")) %>% 
  mutate(country = fct_relevel(fct_lump(country, 7), "US")) %>% 
  mutate(taster_name = fct_relevel(fct_lump(taster_name, 6), "Missing")) %>% 
  lm(points ~ log2(price) + country + year + taster_name, data = .)

model %>% 
  tidy(conf.int = TRUE) %>% # из пакета broom
  filter(term != "(Intercept)") %>% 
  mutate(term = str_replace(term, "country", "Country: "),
         term = str_replace(term, "taster_name", "Taster: "),
         term = fct_reorder(term, estimate)) %>% 
  ggplot(aes(estimate, term)) + 
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high))
```

На графике выше изображены доверительные интервалы для предикторов.

Построим актуальные значения, и на этот же график нанесём предсказанные значения

```{r}
model %>% 
  augment(data = wine_ratings) %>% 
  ggplot(aes(.fitted, points)) + 
  geom_point(alpha = .1)

tidy(anova(model))
```

### Лассо регрессия на словах в описании

Подробнее о методе лассо регрессии можно узнать [тут](https://ranalytics.github.io/data-mining/01-Data-Mining-Models-in-R.html#section_1_1)

```{r}
wine_ratings_words <- wine_ratings %>% 
  mutate(wine_id = row_number()) %>% 
  unnest_tokens(word, description) %>% 
  anti_join(stop_words, by = "word") %>% 
  filter(!word %in% c("wine", "drink"),
         str_detect(word, "[a-z]"))
```

`stop_words` --- это список стоп-слов, которые могут мешать поисковой системе или обучаемой модели. Подробнее в справке.

Посмотрим как часто встречается то или иное слово в описании вина

```{r}
wine_ratings_words %>% 
  count(word, sort = TRUE) %>% 
  head(20) %>% 
  mutate(word = fct_reorder(word, n)) %>% 
  ggplot(aes(word, n)) + 
  geom_col() +
  coord_flip()
```

Теперь посмотрим как часто встречается пара слов в одном описании

```{r}
wine_ratings_words_cors <- wine_ratings_words %>% 
  distinct(wine_id, word) %>% 
  add_count(word) %>% 
  filter(n >= 1000) %>% 
  pairwise_cor(word, wine_id, sort = TRUE)

wine_ratings_words_cors
```

Отфильтруем уникальные значения

```{r}
wine_words_filtered <- wine_ratings_words %>% 
  ?distinct(wine_id, word) %>% 
  add_count(word) %>% 
  filter(n >= 100)

wine_words_filtered
```



```{r}
library(Matrix)

wine_words_matrix <- wine_words_filtered %>% 
  cast_sparse(wine_id, word)

wine_ids <- as.integer(rownames(wine_words_matrix))
scores <- wine_ratings$points[wine_ids]

wine_words_matrix_extra <- cbind(wine_words_matrix, log_price = log2(wine_ratings$price[wine_ids]))

library(glmnet)

glmnet_model <- glmnet(wine_words_matrix, scores)

glmnet_model %>% 
  tidy() %>% 
  filter(term %in% c("rich", "black", "simple")) %>% 
  ggplot(aes(lambda, estimate, color = term)) +
  geom_line() +
  scale_x_log10()
  
glmnet_model %>% 
  tidy() %>% 
  count(lambda) %>% 
  ggplot(aes(lambda, n)) +
  geom_line() +
  scale_x_log10()

cv_glmnet_model <- cv.glmnet(wine_words_matrix, scores) # cross validation

library(doMC)

registerDoMC(cores = 4)

cv_glmnet_model <- cv.glmnet(wine_words_matrix_extra, scores, parallel = TRUE)

plot(cv_glmnet_model)
```

Посмотрим какие слова встречались чаще всего в положительных описаниях

```{r}
cv_glmnet_model$glmnet.fit %>% 
  tidy() %>% 
  filter(lambda == cv_glmnet_model$lambda.1se,
         term != "(Intercept)") %>% 
  arrange(desc(estimate))
```

И в негативных

```{r}
cv_glmnet_model$glmnet.fit %>% 
  tidy() %>% 
  filter(lambda == cv_glmnet_model$lambda.1se,
         term != "(Intercept)") %>% 
  arrange(estimate)
```

Теперь построим график, который отображает по десять самых позитивных и самых негативных слов

```{r}
cv_glmnet_model$glmnet.fit %>% 
  tidy() %>%
  filter(lambda == cv_glmnet_model$lambda.1se,
         term != "(Intercept)",
         term != "log_price") %>%
  arrange(estimate) %>% 
  group_by(direction = ifelse(estimate < 0, "Negative", "Positive")) %>% 
  top_n(10, abs(estimate)) %>% 
  ungroup() %>% 
  mutate(term = fct_reorder(term, estimate)) %>% 
  ggplot(aes(term, estimate, fill = direction)) +
  geom_col() +
  coord_flip() +
  labs(y = "Ожидаемый влияние слова на предсказание")
```

Облегчим код, разобъём его на две части

```{r}
lexicon <- cv_glmnet_model$glmnet.fit %>% 
  tidy() %>%
  filter(lambda == cv_glmnet_model$lambda.1se,
         term != "(Intercept)",
         term != "log_price") %>% 
  select(word = term, coefficient = estimate)

lexicon %>% 
  arrange(coefficient) %>% 
  group_by(direction = ifelse(coefficient < 0, "Negative", "Positive")) %>% 
  top_n(16, abs(coefficient)) %>% 
  ungroup() %>% 
  mutate(word = fct_reorder(word, coefficient)) %>% 
  ggplot(aes(word, coefficient, fill = direction)) +
  geom_col() +
  coord_flip() +
  labs(y = "Ожидаемый влияние слова на предсказание")
```

Посмотрим какие слова встречаются в первом описании

```{r}
wine_ratings_words %>% 
  filter(wine_id == 1) %>% 
  select(word, points)

wine_ratings$description[1]
```

Посмотрим какую оценку имеет каждое не стоп слово в нашем заведённом лексиконе.

```{r}
wine_ratings_words %>% 
  filter(wine_id == 1) %>% 
  select(word, points) %>% 
  inner_join(lexicon, by = "word")
```

Видно, что в целом позитивный отзыв имеет два слова которые имеют обычно негативный эффект. Теперь найдём самый несбалансированный отзыв

```{r}
wine_ratings %>% 
  mutate(wine_id = row_number()) %>% 
  arrange(points) %>% 
  head(1) %>% 
  select(wine_id, description)
```
 
Это 319 по счёту отзыв. Построим для него график, положительных и негавтиных слов отзыва

```{r}
wine_ratings_words %>% 
  filter(wine_id == 319) %>% 
  select(word, points) %>% 
  inner_join(lexicon, by = "word") %>% 
  mutate(word = fct_reorder(word, coefficient)) %>% 
  ggplot(aes(word, coefficient)) +
  geom_col() +
  coord_flip()
```

```{r}
lm_wine <- wine_ratings %>% 
  replace_na(list(taster_name = "Missing", country = "Missing")) %>% 
  mutate(country = fct_relevel(fct_lump(country, 7), "US")) %>% 
  mutate(taster_name = fct_relevel(fct_lump(taster_name, 6), "Missing")) %>% 
  mutate(variety = fct_relevel(fct_lump(variety, 200), "Other")) %>%
  select(points, country, price, year, variety) %>%
  filter(!is.na(year))

lm_wine_model <- lm(points ~ log2(price) + country + year + variety, data = lm_wine)

lm_wine$predict <- predict(object = lm_wine_model, type = "response")




lm_wine <- lm_wine %>%
 mutate(mark = ifelse(abs(predict - points) <= 5, "good", "bad"))
  
lm_wine %>%
  ggplot(aes(log2(price), predict, color = mark)) +
  geom_point() + 
  facet_grid(~ mark)
```

График выше, показывает, что наша модель не очень-то хорошо предсказывает оценку.


```{r}
x <- rnorm(15)
y <- x + rnorm(15)
predict(lm(y ~ x))
```
